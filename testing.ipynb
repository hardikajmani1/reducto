{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from reducto.data_loader import dump_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '/home/lucifer/Documents/Uchicago/winter 22/practicum/dataset'\n",
    "names        = ['auburn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = {\n",
    "    name: {\n",
    "        subset.name: [\n",
    "            segment.name\n",
    "            for segment\n",
    "            in sorted((Path(dataset_root) / name / subset).iterdir())\n",
    "            if segment.match('segment???.mp4')]\n",
    "        for subset in [\n",
    "            s\n",
    "            for s\n",
    "            in sorted((Path(dataset_root) / name).iterdir())\n",
    "            if s.is_dir()\n",
    "        ]\n",
    "    }\n",
    "    for name in names\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auburn': {'raw000': ['segment001.mp4']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(video_list, 'video_list.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vidoer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from reducto.differencer import PixelDiff, AreaDiff, CornerDiff, EdgeDiff\n",
    "from reducto.videoer import Videoer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/lucifer/Documents/Uchicago/winter 22/practicum/dataset/auburn/raw000/segment001.mp4')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_root = '/home/lucifer/Documents/Uchicago/winter 22/practicum/dataset'\n",
    "dataset_name = 'auburn'\n",
    "subset_pattern = 'raw000'\n",
    "segment_root = Path(dataset_root) / dataset_name / subset_pattern\n",
    "segments = [f for f in sorted(segment_root.iterdir()) if f.match('segment???.mp4')]\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting transmitting...\n"
     ]
    }
   ],
   "source": [
    "videoer = Videoer(dataset_root=dataset_root,\n",
    "                      dataset_name=dataset_name,\n",
    "                      subset_pattern=subset_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = [\n",
    "        PixelDiff(thresh=0.01),\n",
    "        AreaDiff(thresh=0.01),\n",
    "        CornerDiff(thresh=0.01),\n",
    "        EdgeDiff(thresh=0.01)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segment001,diff_vector,pixel,1643915900.0979946,1643915907.1175659,7.019571304321289\n",
      "segment001,diff,pixel,1643915907.1183176,1643915907.1183968,7.915496826171875e-05\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for dp in dps:\n",
    "        sent = videoer.send_next(dp)\n",
    "        while sent is True:\n",
    "            sent = videoer.send_next(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "\n",
    "import mongoengine\n",
    "import yaml\n",
    "\n",
    "from reducto.data_loader import dump_json\n",
    "from reducto.differencer import DiffComposer\n",
    "from reducto.evaluator import MetricComposer\n",
    "from reducto.inferencer import Yolo\n",
    "from reducto.model import Segment, Inference, InferenceResult, DiffVector, FrameEvaluation\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'environs': {'dataset_root': '/home/lucifer/Documents/Uchicago/winter 22/practicum/dataset',\n",
       "  'thresh_root': 'config/threshes',\n",
       "  'dataset_name': 'auburn',\n",
       "  'subsets': ['raw000']},\n",
       " 'mongo': {'host': 'localhost', 'port': 27017},\n",
       " 'inference': {'type': 'yolo'},\n",
       " 'differencer': {'use_dict': True, 'types': ['pixel']},\n",
       " 'motioner': {'type': 'adapative_bg_learning'},\n",
       " 'evaluator': [{'type': 'coco', 'class': [0]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = 'pipelines/pipeline-auburn-testing.yaml'\n",
    "with open(configuration, 'r') as y:\n",
    "    config = yaml.load(y, Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/lucifer/Documents/Uchicago/winter 22/practicum/dataset/auburn/raw000/segment001.mp4')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets = ['raw000']\n",
    "segments = []\n",
    "segment_pattern = 'segment???.mp4'\n",
    "for ss in subsets:\n",
    "    p = Path(dataset_root) / dataset_name / ss\n",
    "    segments += [f for f in sorted(p.iterdir()) if f.match(segment_pattern)]\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to localhost:27017 on dataset auburn\n"
     ]
    }
   ],
   "source": [
    "mongo_host = config['mongo']['host']\n",
    "mongo_port = config['mongo']['port']\n",
    "mongoengine.connect(dataset_name, host=mongo_host, port=mongo_port)\n",
    "print(f'connected to {mongo_host}:{mongo_port} on dataset {dataset_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "differ_dict_path = Path(config['environs']['thresh_root']) / f'{dataset_name}.json'\n",
    "differ_types = config['differencer']['types']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucifer/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "Restoring parameters from weights/yolov3/yolov3.ckpt\n"
     ]
    }
   ],
   "source": [
    "# component preparation\n",
    "no_session = False\n",
    "model = Yolo(no_session=no_session)\n",
    "differ = DiffComposer.from_jsonfile(differ_dict_path, differ_types)\n",
    "evaluator = MetricComposer.from_json(config['evaluator'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_diffeval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/metrics_composer.py\", line 44, in evaluate_frame_pair\n    return self.evaluate_single_frame(inference[pair[0]], inference[pair[1]], metric)\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/metrics_composer.py\", line 36, in evaluate_single_frame\n    res = metric.evaluate_single_frame(ground_truth, comparision)\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/metrics.py\", line 17, in evaluate_single_frame\n    return self({'1': ground_truth}, {'1': detection})\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/metrics.py\", line 124, in __call__\n    return self.evaluate()\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/metrics.py\", line 20, in evaluate\n    results = self._evaluate()\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/utils.py\", line 61, in newf\n    return f(*args, **kwargs)\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/metrics.py\", line 151, in _evaluate\n    results = self.evaluator.evaluate()\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/coco/coco_evaluation.py\", line 206, in evaluate\n    box_evaluator = coco_tools.COCOEvalWrapper(\n  File \"/home/lucifer/Documents/Uchicago/winter 22/practicum/reducto/reducto/evaluator/coco/coco_tools.py\", line 168, in __init__\n    cocoeval.COCOeval.__init__(self, groundtruth, detections,\n  File \"/home/lucifer/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py\", line 76, in __init__\n    self.params = Params(iouType=iouType) # parameters\n  File \"/home/lucifer/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py\", line 521, in __init__\n    self.setDetParams()\n  File \"/home/lucifer/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py\", line 501, in setDetParams\n    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n  File \"<__array_function__ internals>\", line 180, in linspace\n  File \"/home/lucifer/.local/lib/python3.8/site-packages/numpy/core/function_base.py\", line 120, in linspace\n    num = operator.index(num)\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aaf179c8f28a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0meval_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_frame_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mmetric_evaluations_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_pending\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         pair_evaluations_new = {\n\u001b[1;32m     67\u001b[0m             \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# pipeline running\n",
    "pbar = tqdm(total=len(segments))\n",
    "for segment in segments:\n",
    "\n",
    "    # -- segment ---------------------------------------------------\n",
    "    segment_record = Segment.find_or_save(segment.parent.name, segment.name)\n",
    "\n",
    "    # -- inference -------------------------------------------------\n",
    "    inference_record = Inference.objects(\n",
    "        segment=segment_record,\n",
    "        model=model.name,\n",
    "    ).first()\n",
    "    if inference_record:\n",
    "        inference = inference_record.to_json()\n",
    "    else:\n",
    "        inference = model.infer_video(segment)\n",
    "        inference_record = Inference(\n",
    "            segment=segment_record,\n",
    "            model=model.name,\n",
    "            result=[InferenceResult.from_json(inf) for _, inf in inference.items()],\n",
    "        )\n",
    "        inference_record.save()\n",
    "    dump_json(inference, f'data/inference/{dataset_name}/{segment.parent.name}/{segment.stem}.json', mkdir=True)\n",
    "\n",
    "    # -- skip if required ------------------------------------------\n",
    "    if skip_diffeval:\n",
    "        pbar.update()\n",
    "        continue\n",
    "\n",
    "    # -- diff ------------------------------------------------------\n",
    "    diff_vectors = {\n",
    "        dv_record.differencer: dv_record.vector\n",
    "        for dv_record in DiffVector.objects(segment=segment_record)\n",
    "    }\n",
    "    differ_types_pending = [d for d in differ_types if d not in diff_vectors]\n",
    "    with mp.Pool() as pool:\n",
    "        dv_f = functools.partial(differ.get_diff_vector, filepath=segment)\n",
    "        diff_vectors_new = pool.map(dv_f, differ_types_pending)\n",
    "    diff_vectors_new = {\n",
    "        differ_type: vector\n",
    "        for differ_type, vector in zip(differ_types_pending, diff_vectors_new)\n",
    "    }\n",
    "    for differ_type, vector in diff_vectors_new.items():\n",
    "        diff_vector_record = DiffVector(\n",
    "            segment=segment_record,\n",
    "            differencer=differ_type,\n",
    "            vector=vector,\n",
    "        )\n",
    "        diff_vector_record.save()\n",
    "\n",
    "    diff_vectors = {**diff_vectors, **diff_vectors_new}\n",
    "    diff_results = differ.process_video(segment, diff_vectors)\n",
    "    dump_json(diff_results, f'data/diff/{dataset_name}/{segment.parent.name}/{segment.stem}.json', mkdir=True)\n",
    "\n",
    "    # -- evaluation ------------------------------------------------\n",
    "    frame_pairs = evaluator.get_frame_pairs(inference, diff_results)\n",
    "\n",
    "    per_frame_evaluations = {}\n",
    "    for metric in evaluator.keys:\n",
    "        metric_evaluations = FrameEvaluation.objects(segment=segment_record, evaluator=metric)\n",
    "        pairs = [(me.ground_truth, me.comparision) for me in metric_evaluations]\n",
    "        pairs_pending = [p for p in frame_pairs if p not in pairs]\n",
    "        with mp.Pool() as pool:\n",
    "            eval_f = functools.partial(evaluator.evaluate_frame_pair, inference=inference, metric=metric)\n",
    "            metric_evaluations_new = pool.map(eval_f, pairs_pending)\n",
    "        pair_evaluations_new = {\n",
    "            pair: evaluation\n",
    "            for pair, evaluation in zip(pairs_pending, metric_evaluations_new)\n",
    "        }\n",
    "        for pair, evaluation in pair_evaluations_new.items():\n",
    "            frame_evaluation_record = FrameEvaluation(\n",
    "                segment=segment_record,\n",
    "                model=model.name,\n",
    "                evaluator=metric,\n",
    "                ground_truth=pair[0],\n",
    "                comparision=pair[1],\n",
    "                result=evaluation[metric],\n",
    "            )\n",
    "            frame_evaluation_record.save()\n",
    "        for me in metric_evaluations:\n",
    "            if not per_frame_evaluations.get((me.ground_truth, me.comparision), None):\n",
    "                per_frame_evaluations[(me.ground_truth, me.comparision)] = {}\n",
    "            per_frame_evaluations[(me.ground_truth, me.comparision)][metric] = me.result\n",
    "        for pair, evaluation in pair_evaluations_new.items():\n",
    "            if not per_frame_evaluations.get(pair, None):\n",
    "                per_frame_evaluations[pair] = {}\n",
    "            per_frame_evaluations[pair][metric] = evaluation[metric]\n",
    "\n",
    "    evaluations = evaluator.evaluate(inference, diff_results, per_frame_evaluations, segment)\n",
    "    dump_json(evaluations, f'data/evaluation/{dataset_name}/{segment.parent.name}/{segment.stem}.json', mkdir=True)\n",
    "\n",
    "    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
